# Fine Words: AI-Powered Language Learning Vocabulary Builder
# Fine Words: AI é©±åŠ¨çš„è¯­è¨€å­¦ä¹ è¯æ±‡æ„å»ºå™¨

[English](#english) | [ä¸­æ–‡](#chinese)

---

<a name="english"></a>
## ğŸ‡¬ğŸ‡§ English Introduction

**Fine Words** is a powerful tool designed to help language learners master foreign languages by analyzing real-world usage frequencies. It fetches massive text datasets from **Hugging Face**, extracts high-frequency words and phrases (n-grams), and uses **Large Language Models (LLMs)** to provide concise, context-aware translations and explanations.

### Key Features
- **Data Streaming**: Efficiently streams large datasets from Hugging Face without downloading the entire dataset.
- **N-gram Extraction**: Generates lists for **Unigrams** (single words), **Bigrams** (2-word phrases), and **Trigrams** (3-word phrases).
- **Dual Processing Modes**:
  - **CPU Mode**: Fast, multi-threaded processing using lightweight models (`en_core_web_sm`).
  - **GPU Mode**: High-accuracy processing using Transformer-based models (`en_core_web_trf`) with CUDA acceleration.
- **LLM Integration**: Automatically translates and explains the extracted vocabulary using OpenAI-compatible APIs (e.g., DeepSeek, GPT-4, etc.).
- **Customizable**: Configurable sample sizes, frequency thresholds, and models via environment variables.

### Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/yourusername/fine-words.git
   cd fine-words
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Download spaCy models**:
   - For CPU mode:
     ```bash
     python -m spacy download en_core_web_sm
     ```
   - For GPU mode (requires CUDA):
     ```bash
     pip install spacy-transformers
     python -m spacy download en_core_web_trf
     ```

### Configuration

Create a `.env` file in the root directory (copy from `.env.example` if available) and configure your settings:

```ini
# Hugging Face Settings
DATA_NAME=wikitext          # Dataset name (e.g., wikitext, c4)
HF_ENDPOINT=https://hf-mirror.com

# Processing Settings
TOP_UNIGRAMS=1000           # Number of top words to save
TOP_BIGRAMS=100             # Number of top 2-word phrases
TOP_TRIGRAMS=100            # Number of top 3-word phrases
SAMPLE_SIZE=10000           # Number of documents to process
MAX_WORKERS=8               # CPU threads
BATCH_SIZE=200              # GPU batch size

# LLM Settings
OPENAI_API_KEY=sk-......
OPENAI_BASE_URL=https://api.deepseek.com
MODEL_NAME=deepseek-chat
```

### Usage

#### Step 1: Generate Frequency Lists

**Option A: CPU Mode (Fast & Lightweight)**
Best for quick analysis on standard hardware.
```bash
python hf-words-cpu.py
```

**Option B: GPU Mode (High Accuracy)**
Best for deep linguistic analysis using Transformers (requires NVIDIA GPU).
```bash
python hf-words-gpu.py
```

*Output: `words.csv`, `phrases.csv`, `trigrams.csv`*

#### Step 2: AI Explanation & Translation

Use an LLM to translate and explain the generated word list.
```bash
python llm-explain.py
```

*Output: `words_explained.csv` containing the original words and their AI-generated translations.*

---

<a name="chinese"></a>
## ğŸ‡¨ğŸ‡³ ä¸­æ–‡ä»‹ç»

**Fine Words** æ˜¯ä¸€ä¸ªæ—¨åœ¨é€šè¿‡åˆ†æçœŸå®è¯­å¢ƒè¯é¢‘æ¥è¾…åŠ©è¯­è¨€å­¦ä¹ çš„å¼ºå¤§å·¥å…·ã€‚å®ƒä» **Hugging Face** è·å–æµ·é‡æ–‡æœ¬æ•°æ®é›†ï¼Œæå–é«˜é¢‘å•è¯å’ŒçŸ­è¯­ï¼ˆN-gramsï¼‰ï¼Œå¹¶åˆ©ç”¨ **å¤§è¯­è¨€æ¨¡å‹ (LLMs)** æä¾›ç®€æ´ã€å‡†ç¡®çš„ä¸­æ–‡ç¿»è¯‘å’Œè§£é‡Šã€‚

### ä¸»è¦åŠŸèƒ½
- **æµå¼æ•°æ®å¤„ç†**: é«˜æ•ˆæµå¼ä¼ è¾“ Hugging Face å¤§å‹æ•°æ®é›†ï¼Œæ— éœ€ä¸‹è½½å®Œæ•´æ•°æ®ã€‚
- **å¤šçº§è¯ç»„æå–**: ç”Ÿæˆ **å•è¯ (Unigrams)**ã€**åŒè¯çŸ­è¯­ (Bigrams)** å’Œ **ä¸‰è¯çŸ­è¯­ (Trigrams)** åˆ—è¡¨ã€‚
- **åŒé‡å¤„ç†æ¨¡å¼**:
  - **CPU æ¨¡å¼**: ä½¿ç”¨è½»é‡çº§æ¨¡å‹ (`en_core_web_sm`) è¿›è¡Œå¿«é€Ÿã€å¤šçº¿ç¨‹å¤„ç†ã€‚
  - **GPU æ¨¡å¼**: ä½¿ç”¨åŸºäº Transformer çš„æ¨¡å‹ (`en_core_web_trf`) å’Œ CUDA åŠ é€Ÿï¼Œå®ç°é«˜ç²¾åº¦åˆ†æã€‚
- **LLM é›†æˆ**: è°ƒç”¨ OpenAI å…¼å®¹æ¥å£ï¼ˆå¦‚ DeepSeek, GPT-4 ç­‰ï¼‰è‡ªåŠ¨ç¿»è¯‘å’Œè§£é‡Šæå–çš„è¯æ±‡ã€‚
- **é«˜åº¦å¯é…ç½®**: å¯é€šè¿‡ç¯å¢ƒå˜é‡è‡ªå®šä¹‰æ ·æœ¬å¤§å°ã€é¢‘ç‡é˜ˆå€¼å’Œä½¿ç”¨çš„æ¨¡å‹ã€‚

### å®‰è£…æŒ‡å—

1. **å…‹éš†ä»“åº“**:
   ```bash
   git clone https://github.com/yourusername/fine-words.git
   cd fine-words
   ```

2. **å®‰è£…ä¾èµ–**:
   ```bash
   pip install -r requirements.txt
   ```

3. **ä¸‹è½½ spaCy æ¨¡å‹**:
   - CPU æ¨¡å¼:
     ```bash
     python -m spacy download en_core_web_sm
     ```
   - GPU æ¨¡å¼ (éœ€è¦ CUDA):
     ```bash
     pip install spacy-transformers
     python -m spacy download en_core_web_trf
     ```

### é…ç½®è¯´æ˜

åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆ›å»º `.env` æ–‡ä»¶ï¼Œå¹¶é…ç½®ä»¥ä¸‹å‚æ•°ï¼š

```ini
# Hugging Face è®¾ç½®
DATA_NAME=wikitext          # æ•°æ®é›†åç§° (å¦‚ wikitext, c4)
HF_ENDPOINT=https://hf-mirror.com

# å¤„ç†è®¾ç½®
TOP_UNIGRAMS=1000           # ä¿å­˜çš„é«˜é¢‘è¯æ•°é‡
TOP_BIGRAMS=100             # ä¿å­˜çš„é«˜é¢‘åŒè¯çŸ­è¯­æ•°é‡
TOP_TRIGRAMS=100            # ä¿å­˜çš„é«˜é¢‘ä¸‰è¯çŸ­è¯­æ•°é‡
SAMPLE_SIZE=10000           # å¤„ç†çš„æ–‡æ¡£æ ·æœ¬æ•°é‡
MAX_WORKERS=8               # CPU çº¿ç¨‹æ•°
BATCH_SIZE=200              # GPU æ‰¹å¤„ç†å¤§å°

# LLM è®¾ç½®
OPENAI_API_KEY=sk-......    # API å¯†é’¥
OPENAI_BASE_URL=https://api.deepseek.com
MODEL_NAME=deepseek-chat    # æ¨¡å‹åç§°
```

### ä½¿ç”¨æ–¹æ³•

#### ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆè¯é¢‘è¡¨

**é€‰é¡¹ A: CPU æ¨¡å¼ (å¿«é€Ÿ & è½»é‡)**
é€‚åˆåœ¨æ™®é€šç¡¬ä»¶ä¸Šå¿«é€Ÿåˆ†æã€‚
```bash
python hf-words-cpu.py
```

**é€‰é¡¹ B: GPU æ¨¡å¼ (é«˜ç²¾åº¦)**
é€‚åˆä½¿ç”¨ Transformer æ¨¡å‹è¿›è¡Œæ·±åº¦è¯­è¨€åˆ†æï¼ˆéœ€è¦ NVIDIA GPUï¼‰ã€‚
```bash
python hf-words-gpu.py
```

*ç”Ÿæˆæ–‡ä»¶: `words.csv`, `phrases.csv`, `trigrams.csv`*

#### ç¬¬äºŒæ­¥ï¼šAI ç¿»è¯‘ä¸è§£é‡Š

ä½¿ç”¨ LLM ä¸ºç”Ÿæˆçš„å•è¯è¡¨æä¾›ç¿»è¯‘å’Œè§£é‡Šã€‚
```bash
python llm-explain.py
```

*ç”Ÿæˆæ–‡ä»¶: `words_explained.csv`ï¼ŒåŒ…å«åŸå§‹å•è¯åŠå…¶ AI ç”Ÿæˆçš„ç¿»è¯‘ã€‚*

